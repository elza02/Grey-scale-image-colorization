{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2118595,"sourceType":"datasetVersion","datasetId":1271215}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\ntf.keras.backend.clear_session()\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.layers import (\n    Input, Conv2D, Conv2DTranspose, LayerNormalization, \n    MultiHeadAttention, Dense, Dropout, Add, Concatenate, \n    BatchNormalization, ReLU, Reshape, Permute, UpSampling2D, Lambda\n)\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.models import Model\nimport numpy as np\nimport os\nfrom skimage.color import rgb2lab\nfrom keras.layers import Resizing\nimport matplotlib.pyplot as plt\nimport os\nfrom skimage import color\n\n# Constants\nLAMBDA_ADVERSARIAL = 0.1\nLAMBDA_PERCEPTUAL = 100\nLAMBDA_L1 = 10\nLAMBDA_COLOR = 1\nGLOBAL_BATCH_SIZE = 16\n\nclass WindowAttention(tf.keras.layers.Layer):\n    def __init__(self, dim, window_size, num_heads, qkv_bias=True):\n        super().__init__()\n        self.dim = dim\n        self.window_size = window_size\n        self.num_heads = num_heads\n        self.scale = (dim // num_heads) ** -0.5\n        self.qkv = Dense(dim * 3, use_bias=qkv_bias)\n        self.proj = Dense(dim)\n\n    def call(self, x):\n        B, H, W, C = x.shape\n        x = Reshape((-1, C))(x)\n        qkv = self.qkv(x)\n        qkv = Reshape((-1, 3, self.num_heads, C // self.num_heads))(qkv)\n        qkv = tf.transpose(qkv, [2, 0, 3, 1, 4])\n        q, k, v = qkv[0], qkv[1], qkv[2]\n        attn = tf.matmul(q, k, transpose_b=True) * self.scale\n        attn = tf.nn.softmax(attn, axis=-1)\n        x = tf.matmul(attn, v)\n        x = tf.transpose(x, [0, 2, 1, 3])\n        x = Reshape((H, W, C))(x)\n        x = self.proj(x)\n        return x\n\ndef swin_transformer_block(x, dim, num_heads, window_size=7):\n    x_norm1 = LayerNormalization(epsilon=1e-5)(x)\n    x_attn = WindowAttention(dim, window_size, num_heads)(x_norm1)\n    x = Add()([x, x_attn])\n    x_norm2 = LayerNormalization(epsilon=1e-5)(x)\n    x_mlp = Dense(dim * 4, activation='gelu')(x_norm2)\n    x_mlp = Dense(dim)(x_mlp)\n    x = Add()([x, x_mlp])\n    return x\n\ndef color_encoder(input_shape=(256, 256, 1)):\n    random_input = Input(shape=(input_shape[0], input_shape[1], 128))\n    x = Conv2D(32, (4, 4), strides=2, padding=\"same\")(random_input)\n    x = BatchNormalization()(x)\n    x = ReLU()(x)\n    x = Conv2D(64, (4, 4), strides=2, padding=\"same\")(x)\n    x = BatchNormalization()(x)\n    x = ReLU()(x)\n    x = Conv2D(128, (4, 4), strides=2, padding=\"same\")(x)\n    x = BatchNormalization()(x)\n    x = ReLU()(x)\n    x = Conv2D(256, (4, 4), strides=2, padding=\"same\")(x)\n    x = BatchNormalization()(x)\n    x = ReLU()(x)\n    return Model(random_input, x, name=\"ColorEncoder\")\n\ndef color_transformer(encoder_output, color_features):\n    x = Concatenate()([encoder_output, color_features])\n    x_initial = Conv2D(256, (3, 3), padding=\"same\")(x)\n    x_initial = BatchNormalization()(x_initial)\n    x_initial = ReLU()(x_initial)\n    x = swin_transformer_block(x_initial, dim=256, num_heads=4)\n    x = swin_transformer_block(x, dim=256, num_heads=4)\n    x = Add()([x, x_initial])\n    return x\n\ndef build_generator(input_shape=(256, 256, 1)):\n    inp = Input(shape=input_shape)\n    \n    # VGG Feature Extractor\n    grayscale_input = Lambda(lambda x: tf.repeat(x, 3, axis=-1))(inp)\n    vgg = VGG19(weights=\"imagenet\", include_top=False)\n    vgg.trainable = False\n    vgg_features = []\n    x = grayscale_input\n    for i, layer in enumerate(vgg.layers):\n        if isinstance(layer, tf.keras.layers.InputLayer):\n            continue\n        x = layer(x)\n        if layer.name in ['block1_conv2', 'block2_conv2', 'block3_conv4', 'block4_conv4']:\n            vgg_features.append(x)\n    \n    random_noise = Lambda(lambda x: tf.random.normal(\n        shape=(tf.shape(x)[0], input_shape[0], input_shape[1], 128),\n        mean=0, stddev=0.1))(inp)\n    \n    color_enc = color_encoder(input_shape)\n    color_features = color_enc(random_noise)\n    \n    # Encoder path with VGG features\n    e1 = Conv2D(32, (4, 4), strides=2, padding=\"same\")(inp)  # 128x128\n    e1 = BatchNormalization()(e1)\n    e1 = ReLU()(e1)\n    vgg1_resized = Conv2D(64, (1, 1))(vgg_features[0])\n    vgg1_resized = Resizing(128, 128)(vgg1_resized)\n    e1 = Concatenate()([e1, vgg1_resized])\n    \n    e2 = Conv2D(64, (4, 4), strides=2, padding=\"same\")(e1)  # 64x64\n    e2 = BatchNormalization()(e2)\n    e2 = ReLU()(e2)\n    vgg2_resized = Conv2D(64, (1, 1))(vgg_features[1])\n    vgg2_resized = Resizing(64, 64)(vgg2_resized) \n    e2 = Concatenate()([e2, vgg2_resized])\n    \n    e3 = Conv2D(128, (4, 4), strides=2, padding=\"same\")(e2)  # 32x32\n    e3 = BatchNormalization()(e3)\n    e3 = ReLU()(e3)\n    vgg3_resized = Conv2D(128, (1, 1))(vgg_features[2])\n    vgg3_resized = Resizing(32, 32)(vgg3_resized)\n    e3 = Concatenate()([e3, vgg3_resized])\n    \n    e4 = Conv2D(256, (4, 4), strides=2, padding=\"same\")(e3)  # 16x16\n    e4 = BatchNormalization()(e4)\n    e4 = ReLU()(e4)\n    vgg4_resized = Conv2D(256, (1, 1))(vgg_features[3])\n    vgg4_resized = Resizing(16, 16)(vgg4_resized)\n    e4 = Concatenate()([e4, vgg4_resized])\n    \n    models = {\n        'vgg': vgg,\n        'color_encoder': color_enc\n    }\n    \n    x = color_transformer(e4, color_features)\n    \n    # Decoder path\n    d1 = Conv2DTranspose(128, (4, 4), strides=2, padding=\"same\")(x)\n    d1 = Concatenate()([d1, e3])\n    d1 = BatchNormalization()(d1)\n    d1 = ReLU()(d1)\n    \n    d2 = Conv2DTranspose(64, (4, 4), strides=2, padding=\"same\")(d1)\n    d2 = Concatenate()([d2, e2])\n    d2 = BatchNormalization()(d2)\n    d2 = ReLU()(d2)\n    \n    d3 = Conv2DTranspose(32, (4, 4), strides=2, padding=\"same\")(d2)\n    d3 = Concatenate()([d3, e1])\n    d3 = BatchNormalization()(d3)\n    d3 = ReLU()(d3)\n    \n    d4 = Conv2DTranspose(16, (4, 4), strides=2, padding=\"same\")(d3)\n    d4 = BatchNormalization()(d4)\n    d4 = ReLU()(d4)\n    \n    output = Conv2D(2, (3, 3), padding=\"same\", activation=\"tanh\")(d4)\n    \n    return Model(inp, output, name=\"Generator\"), models\n\ndef build_discriminator(input_shape=(256, 256, 3)):\n    inp = Input(shape=input_shape)\n    x = Conv2D(64, (4, 4), strides=2, padding=\"same\")(inp)\n    x = BatchNormalization()(x)\n    x = ReLU()(x)\n    x = Conv2D(128, (4, 4), strides=2, padding=\"same\")(x)\n    x = BatchNormalization()(x)\n    x = ReLU()(x)\n    x = Conv2D(256, (4, 4), strides=2, padding=\"same\")(x)\n    x = BatchNormalization()(x)\n    x = ReLU()(x)\n    x = Conv2D(512, (4, 4), strides=2, padding=\"same\")(x)\n    x = BatchNormalization()(x)\n    x = ReLU()(x)\n    output = Conv2D(1, (4, 4), padding=\"same\")(x)\n    return Model(inp, output, name=\"Discriminator\")\n\ndef gradient_penalty(discriminator, l_channel, fake_ab, real_ab):\n    \"\"\"Compute gradient penalty for WGAN-GP\"\"\"\n    batch_size = tf.shape(l_channel)[0]\n    alpha = tf.random.uniform([batch_size, 1, 1, 1], 0., 1.)\n    \n    # Create interpolated images\n    fake_images = tf.concat([l_channel, fake_ab], axis=-1)\n    real_images = tf.concat([l_channel, real_ab], axis=-1)\n    interpolated = alpha * real_images + (1 - alpha) * fake_images\n    \n    with tf.GradientTape() as tape:\n        tape.watch(interpolated)\n        disc_interpolated = discriminator(interpolated, training=True)\n    \n    gradients = tape.gradient(disc_interpolated, interpolated)\n    gradients_sqr = tf.square(gradients)\n    gradients_sqr_sum = tf.reduce_sum(gradients_sqr, axis=[1, 2, 3])\n    gradient_l2_norm = tf.sqrt(gradients_sqr_sum)\n    gradient_penalty = tf.reduce_mean(tf.square(gradient_l2_norm - 1.0))\n    \n    return gradient_penalty\n\ndef preprocess_image(image_path, target_size=(256, 256)):\n    def _convert_to_lab(image_path):\n        # Read and normalize image to [0, 1]\n        image = tf.io.read_file(image_path)\n        image = tf.image.decode_jpeg(image, channels=3)\n        image = tf.image.resize(image, target_size)\n        image = tf.cast(image, tf.float32) / 255.0\n        \n        # Convert to LAB\n        lab_image = color.rgb2lab(image.numpy())\n        \n        # Normalize L to [-1, 1] and ab to [-1, 1]\n        L = (lab_image[..., :1] - 50.0) / 50.0  # Map [0, 100] to [-1, 1]\n        ab = lab_image[..., 1:] / 127.0  # Map [-128, 128] to [-1, 1]\n        \n        return L.astype(np.float32), ab.astype(np.float32)\n    \n    L, ab = tf.py_function(func=_convert_to_lab, inp=[image_path], Tout=[tf.float32, tf.float32])\n    L.set_shape((target_size[0], target_size[1], 1))\n    ab.set_shape((target_size[0], target_size[1], 2))\n    return L, ab\n\n\ndef build_vgg_feature_extractor(input_shape=(256, 256, 3)):\n    \"\"\"Build a VGG feature extractor model that outputs the features we need\"\"\"\n    vgg = VGG19(weights=\"imagenet\", include_top=False, input_shape=input_shape)\n    vgg.trainable = False\n    \n    # Get the specific layer we want\n    layer_name = 'block4_conv4'\n    feature_extractor = Model(\n        inputs=vgg.input,\n        outputs=vgg.get_layer(layer_name).output,\n        name='vgg_feature_extractor'\n    )\n    return feature_extractor\n\n\n\n# Initialize distribution strategy\nstrategy = tf.distribute.MirroredStrategy()\nprint('Number of devices:', strategy.num_replicas_in_sync)\n\n\ndef create_distributed_dataset(image_dir, batch_size=32):  # Add batch_size parameter with default value\n    def load_and_preprocess_image(image_path):\n        # Read image\n        image = tf.io.read_file(image_path)\n        image = tf.image.decode_jpeg(image, channels=3)\n        \n        # Resize\n        image = tf.image.resize(image, [256, 256])\n        \n        # Convert to float32 and normalize to [-1, 1]\n        image = tf.cast(image, tf.float32) / 127.5 - 1\n        \n        # Convert RGB to LAB\n        lab = tf.py_function(lambda x: color.rgb2lab(x), [image], tf.float32)\n        \n        # Split into L and ab channels\n        l_channel = lab[..., :1]\n        ab_channels = lab[..., 1:]\n        \n        return l_channel, ab_channels\n\n    # Get all image paths\n    image_paths = tf.data.Dataset.list_files(str(image_dir + '/*.*'))\n    \n    # Create dataset\n    dataset = image_paths.map(load_and_preprocess_image, \n                            num_parallel_calls=tf.data.AUTOTUNE)\n    \n    # Batch and prefetch\n    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n    \n    return dataset\n    \n\n\n\n@tf.function\ndef lab_to_rgb_tf(lab_image):\n    \"\"\"Convert LAB image to RGB using TensorFlow operations\"\"\"\n    # Denormalize L from [-1, 1] to [0, 100]\n    L = (lab_image[..., 0:1] + 1) * 50\n    # Denormalize ab from [-1, 1] to [-128, 128]\n    ab = lab_image[..., 1:] * 127\n    \n    # Instead of using skimage's lab2rgb, we'll use a simplified conversion\n    # This is an approximation of the LAB to RGB conversion\n    # Note: For more accurate results, you might want to implement the full conversion matrix\n    lab = tf.concat([L, ab], axis=-1)\n    \n    # Normalize to [0, 1] range for VGG\n    rgb = tf.clip_by_value(lab / 255.0, 0, 1)\n    # Repeat the L channel 3 times to create an RGB image\n    rgb = tf.repeat(rgb[..., 0:1], 3, axis=-1)\n    \n    return rgb\ndef train_model(image_dir, epochs=50, checkpoint_dir='checkpoints'):\n    if not os.path.exists(checkpoint_dir):\n        os.makedirs(checkpoint_dir)\n        \n    with strategy.scope():\n        generator, models = build_generator(input_shape=(256, 256, 1))\n        discriminator = build_discriminator(input_shape=(256, 256, 3))\n        vgg_feature_extractor = build_vgg_feature_extractor()\n        color_encoder_model = models['color_encoder']\n        \n        feature_matching_conv = Conv2D(512, (1, 1), padding='same')\n        dummy_input = tf.random.normal([1, 32, 32, 256])\n        _ = feature_matching_conv(dummy_input)\n        \n        gen_optimizer = tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5, beta_2=0.999)\n        disc_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5, beta_1=0.5, beta_2=0.999)\n       \n        def train_step(dist_inputs):\n            l_channel, ab_channels = dist_inputs\n            \n            # Train discriminator n_critic times\n            n_critic = 5\n            for _ in range(n_critic):\n                with tf.GradientTape() as disc_tape:\n                    fake_ab = generator(l_channel, training=True)\n                    fake_images = tf.concat([l_channel, fake_ab], axis=-1)\n                    real_images = tf.concat([l_channel, ab_channels], axis=-1)\n                    \n                    disc_fake = discriminator(fake_images, training=True)\n                    disc_real = discriminator(real_images, training=True)\n                    \n                    disc_loss = tf.reduce_mean(disc_fake) - tf.reduce_mean(disc_real)\n                    gp = gradient_penalty(discriminator, l_channel, fake_ab, ab_channels)\n                    total_disc_loss = (disc_loss + 10.0 * gp) / strategy.num_replicas_in_sync\n                \n                # Calculate and apply discriminator gradients\n                disc_gradients = disc_tape.gradient(total_disc_loss, discriminator.trainable_variables)\n                disc_gradients, _ = tf.clip_by_global_norm(disc_gradients, 1.0)\n                disc_optimizer.apply_gradients(zip(disc_gradients, discriminator.trainable_variables))\n            \n            # Generator training (once)\n            with tf.GradientTape() as gen_tape:\n                fake_ab = generator(l_channel, training=True)\n                fake_images = tf.concat([l_channel, fake_ab], axis=-1)\n                real_images = tf.concat([l_channel, ab_channels], axis=-1)\n                \n                fake_rgb = (fake_images + 1) / 2\n                real_rgb = (real_images + 1) / 2\n                vgg_real = vgg_feature_extractor(real_rgb)\n                vgg_fake = vgg_feature_extractor(fake_rgb)\n                \n                batch_size = tf.shape(l_channel)[0]\n                random_noise = tf.random.normal(shape=(batch_size, 256, 256, 128), mean=0, stddev=0.1)\n                color_encoded_features = color_encoder_model(random_noise)\n                color_encoded_features = tf.image.resize(color_encoded_features, [32, 32])\n                color_encoded_features = feature_matching_conv(color_encoded_features)\n                \n                disc_fake = discriminator(fake_images, training=True)\n                \n                # Generator losses\n                gen_adv_loss = -tf.reduce_mean(disc_fake)\n                perceptual_loss = tf.reduce_mean(tf.abs(vgg_fake - vgg_real))\n                l1_loss = tf.reduce_mean(tf.abs(real_images - fake_images))\n                color_loss = tf.reduce_mean(tf.abs(color_encoded_features - vgg_real))\n            \n                \n                total_gen_loss = (\n                    LAMBDA_ADVERSARIAL * gen_adv_loss +\n                    LAMBDA_PERCEPTUAL * perceptual_loss +\n                    LAMBDA_L1 * l1_loss +\n                    LAMBDA_COLOR * color_loss\n                ) / strategy.num_replicas_in_sync\n            \n            # Calculate and apply generator gradients\n            gen_gradients = gen_tape.gradient(total_gen_loss, generator.trainable_variables)\n            gen_optimizer.apply_gradients(zip(gen_gradients, generator.trainable_variables))\n            \n            return total_gen_loss, total_disc_loss\n\n        @tf.function\n        def distributed_train_step(dist_inputs):\n            per_replica_losses = strategy.run(train_step, args=(dist_inputs,))\n            return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)\n\n    dataset = create_distributed_dataset(image_dir)\n    dist_dataset = strategy.experimental_distribute_dataset(dataset)\n    \n    train_gen_loss = tf.keras.metrics.Mean(name='train_gen_loss')\n    train_disc_loss = tf.keras.metrics.Mean(name='train_disc_loss')\n\n    for epoch in range(epochs):\n        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n        train_gen_loss.reset_state()\n        train_disc_loss.reset_state()\n        \n        for step, dist_inputs in enumerate(dist_dataset):\n            gen_loss, disc_loss = distributed_train_step(dist_inputs)\n            train_gen_loss.update_state(gen_loss)\n            train_disc_loss.update_state(disc_loss)\n            \n            if step % 50 == 0:\n                print(f\"Step {step}: Gen Loss = {train_gen_loss.result():.4f}, \"\n                      f\"Disc Loss = {train_disc_loss.result():.4f}\")\n        \n        if (epoch + 1) % 5 == 0:\n            generator.save_weights(f'generator_epoch_{epoch+1}.weights.h5')\n            discriminator.save_weights(f'discriminator_epoch_{epoch+1}.weights.h5')\n            print(f\"Saved weights for epoch {epoch+1}\")\n        \n        print(f\"Epoch {epoch + 1} Results:\")\n        print(f\"Generator Loss: {train_gen_loss.result():.4f}\")\n        print(f\"Discriminator Loss: {train_disc_loss.result():.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T04:44:05.901689Z","iopub.execute_input":"2025-01-24T04:44:05.902043Z","iopub.status.idle":"2025-01-24T04:44:06.096376Z","shell.execute_reply.started":"2025-01-24T04:44:05.902019Z","shell.execute_reply":"2025-01-24T04:44:06.095376Z"}},"outputs":[{"name":"stdout","text":"Number of devices: 2\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"train_model(\"/kaggle/input/pascal-voc-2012-dataset/VOC2012_train_val/VOC2012_train_val/JPEGImages\",epochs=50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T04:44:19.312149Z","iopub.execute_input":"2025-01-24T04:44:19.312416Z","execution_failed":"2025-01-24T05:04:32.639Z"}},"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n\u001b[1m80134624/80134624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n\nEpoch 1/50\nStep 0: Gen Loss = 811.7355, Disc Loss = 0.1773\nStep 50: Gen Loss = 771.1486, Disc Loss = -1.7468\nStep 100: Gen Loss = 771.1143, Disc Loss = -3.9859\nStep 150: Gen Loss = 770.3538, Disc Loss = -6.8389\nStep 200: Gen Loss = 771.9755, Disc Loss = -10.0045\n","output_type":"stream"}],"execution_count":null}]}